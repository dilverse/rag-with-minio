{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f3193a4e8d8f39",
   "metadata": {},
   "source": [
    "# RAG With MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a25904010ed706",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonstrate how to use MinIO to build a Retrieval Augmented Generation(RAG) based chat application using commodity hardware. \n",
    "* Use MinIO to store all the documents, processed chunks and the embeddings using the vector database.\n",
    "* Use MinIO's bucket notification feature to trigger events when adding or removing documents to a bucket\n",
    "* Webhook that consumes the event and process the documents using Langchain and saves the metadata and chunked documents to a metadata bucket\n",
    "* Trigger MinIO bucket notification events for newly added or removed chunked documents\n",
    "* A Webhook that consumes the events and generates embeddings and save it to the Vector Database (LanceDB) that is persisted in MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f006e48878ddd9f",
   "metadata": {},
   "source": [
    "## Key Tools Used\n",
    "* **MinIO**  - Object Store to persist all the Data\n",
    "* **LanceDB** - Serverless open-source Vector Database that persists data in object store\n",
    "* **Ollama** - To run LLM and embedding model locally (OpenAI API compatible)\n",
    "* **Gradio** - Interface through which to interact with RAG application\n",
    "* **FastAPI** - Server for the Webhooks that receives bucket notification from MinIO and exposes the Gradio App\n",
    "* **LangChain & Unstructured** - To Extract useful text from our documents and Chunk them for Embedding\n",
    "\n",
    "### Models Uses\n",
    "* **LLM** - Phi-3-128K (3.8B Parameters)\n",
    "* **Embeddings** - Nomic Embed Text v1.5 ([Matryoshka Embeddings](https://arxiv.org/pdf/2205.13147)/ 768 Dim, 8K context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57cc767986b0",
   "metadata": {},
   "source": [
    "### Start MinIO Server \n",
    "\n",
    "You can download the binary if you don't have it already from [here](https://min.io/docs/minio/macos/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1b303-37ac-4f40-84cb-f447d2731828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MinIO detached\n",
    "!minio server ~/dev/data --console-address :9090 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394221998bbe143",
   "metadata": {},
   "source": [
    "### Start Ollama Server + Download LLM & Embedding Model\n",
    "\n",
    "Download Ollama from [here](https://ollama.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be323ad2-1aa6-4132-b873-172dfe38d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Server\n",
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5473a5-1b82-46ad-8635-119b7e479849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download Phi-3 LLM\n",
    "!ollama pull phi3:3.8b-mini-128k-instruct-q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0e03e-e962-48b1-ab77-1951b1082cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Nomic Embed Text v1.5\n",
    "!ollama pull nomic-embed-text:v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a5cb1-5fbf-4835-9999-bcec1010963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List All the Models\n",
    "!ollama ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee95f1-0b1d-48ea-add1-385c760f95ac",
   "metadata": {},
   "source": [
    "### Create A Basic Gradio App Using FastAPI to Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbce54-8542-4312-8092-7a7218eab32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"phi3:3.8b-mini-128k-instruct-q8_0\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text:v1.5\"\n",
    "LLM_ENDPOINT = \"http://localhost:11434/api/chat\"\n",
    "CHAT_API_PATH = \"/chat\"\n",
    "\n",
    "\n",
    "def llm_chat(user_question, history):\n",
    "    history = history or []\n",
    "    user_message = f\"**You**: {user_question}\"\n",
    "    llm_resp = requests.post(LLM_ENDPOINT,\n",
    "                             json={\"model\": LLM_MODEL,\n",
    "                                   \"keep_alive\": \"48h\", # Keep the model in-memory for 48 hours\n",
    "                                   \"messages\": [\n",
    "                                       {\"role\": \"user\",\n",
    "                                        \"content\": user_question\n",
    "                                        }\n",
    "                                   ]},\n",
    "                             stream=True)\n",
    "    bot_response = \"**AI:** \"\n",
    "    for resp in llm_resp.iter_lines():\n",
    "        json_data = json.loads(resp)\n",
    "        bot_response += json_data[\"message\"][\"content\"]\n",
    "        yield bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f2671-6cf0-41f4-973b-a8c856acbf78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "import requests\n",
    "from fastapi import FastAPI, Request, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "with gr.Blocks(gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## RAG with MinIO\")\n",
    "    ch_interface = gr.ChatInterface(llm_chat, undo_btn=None, clear_btn=\"Clear\")\n",
    "    ch_interface.chatbot.show_label = False\n",
    "    ch_interface.chatbot.height = 600\n",
    "\n",
    "demo.queue()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    app = gr.mount_gradio_app(app, demo, path=CHAT_API_PATH)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8808)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0166ee-7b68-4278-a48d-4a64fd94502e",
   "metadata": {},
   "source": [
    "### Test Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4455955-4eb8-4bb3-9d96-7a4c5fceee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_ENDPOINT = \"http://localhost:11434/api/embeddings\"\n",
    "EMBEDDINGS_DIM = 768\n",
    "\n",
    "def get_embedding(text):\n",
    "    resp = requests.post(EMBEDDING_ENDPOINT,\n",
    "                         json={\"model\": EMBEDDING_MODEL,\n",
    "                               \"prompt\": text})\n",
    "    return np.array(resp.json()[\"embedding\"][:EMBEDDINGS_DIM], dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0ca53-fac0-4aa0-91d3-19485f25eaaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Test with sample text\n",
    "get_embedding(\"What is MinIO?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "896f38e7-d018-4631-a1b7-d40e25815151",
   "metadata": {},
   "source": [
    "## Ingestion Pipeline Overview\n",
    "\n",
    "![MinIO-RAG-Ingest.png](media/rag-minio.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c6d47-07f2-49e4-8088-a8667f4ca239",
   "metadata": {},
   "source": [
    "## Create MinIO Buckets\n",
    "\n",
    "Use `mc` command or do it from UI\n",
    "\n",
    "* **custom-corpus** - To store all the documents\n",
    "* **warehouse** - To store all the metadata, chunks and vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0bdf7d-a337-4ed7-bb8b-1ed56157df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc alias set 'myminio' 'http://localhost:9000' 'minioadmin' 'minioadmin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee6d39-f75d-40e9-8adf-8061945f72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc mb myminio/custom-corpus\n",
    "!mc mb myminio/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0914d-ca8f-41c3-b900-c339c03eb9f9",
   "metadata": {},
   "source": [
    "### Create Webhook that Consumes Bucket Notifications from `custom-corpus` bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ea2a1-c2c5-42fc-b17a-eaebc19ed522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "import requests\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/api/v1/document/notification\")\n",
    "async def receive_webhook(request: Request):\n",
    "    json_data = await request.json()\n",
    "    print(json.dumps(json_data, indent=2))\n",
    "\n",
    "with gr.Blocks(gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## RAG with MinIO\")\n",
    "    ch_interface = gr.ChatInterface(llm_chat, undo_btn=None, clear_btn=\"Clear\")\n",
    "    ch_interface.chatbot.show_label = False\n",
    "\n",
    "demo.queue()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    app = gr.mount_gradio_app(app, demo, path=CHAT_API_PATH)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8808)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317abd7-7577-4ad0-9a99-bf28a1b8dbc3",
   "metadata": {},
   "source": [
    "### Create MinIO Event Notifications and Link it to `custom-corpus` Bucket\n",
    "\n",
    "### Creat Webhook Event\n",
    "\n",
    "In Console go to `Events`-> `Add Event Destination` -> `Webhook`\n",
    "\n",
    "Fill the fields with Following values and hit save\n",
    "\n",
    "**Identifier** - doc-webhook\n",
    "\n",
    "**Endpoint** - http://localhost:8808/api/v1/document/notification\n",
    "\n",
    "Click `Restart MinIO` at the top when pormpted to\n",
    "\n",
    "(**Note**: You can also use `mc` for this)\n",
    "\n",
    "### Link the Webhook Event to `custom-corpus` bucket Events\n",
    "\n",
    "In console go to `Buckets (Administrator)` -> `custom-corpus` -> `Events`\n",
    "\n",
    "Fill the fields with Following values and hit save\n",
    "\n",
    "**ARN** - Select the `doc-webhook` from dropdown\n",
    "\n",
    "**Select Events** - Check `PUT` and `DELETE`\n",
    "\n",
    "(**Note**: You can also use `mc` for this)\n",
    "\n",
    "We have our first webhook setup\n",
    "\n",
    "#### Now test by adding and removing an object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97dc0ff-d2f8-4fe0-890d-1037968d65ec",
   "metadata": {},
   "source": [
    "## Extract data from the Documents and Chunk\n",
    "\n",
    "We will use `Langchain` and `Unstructured` to read an object from MinIO and Split Documents in to multiples chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fca0d-7e13-4c7d-a4e1-d65740ccc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import S3FileLoader\n",
    "\n",
    "MINIO_ENDPOINT = \"http://localhost:9000\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin\"\n",
    "\n",
    "\n",
    "# Split Text from a given document using chunk_size number of characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=64,\n",
    "                                               length_function=len)\n",
    "\n",
    "\n",
    "def split_doc_by_chunks(bucket_name, object_key):\n",
    "    loader = S3FileLoader(bucket_name,\n",
    "                          object_key,\n",
    "                          endpoint_url=MINIO_ENDPOINT,\n",
    "                          aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "                          aws_secret_access_key=MINIO_SECRET_KEY)\n",
    "    docs = loader.load()\n",
    "    doc_splits = text_splitter.split_documents(docs)\n",
    "    return doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159b328-6648-4af0-b026-d448113d8dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test the chunking\n",
    "split_doc_by_chunks(\"custom-corpus\", \"The-Enterprise-Object-Store-Feature-Set.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afef3f-63ed-4c78-9cab-2dac861cd35c",
   "metadata": {},
   "source": [
    "### Add the Chunking logic to Webhook\n",
    "\n",
    "Add the chunk logic to webhook and save the metadata and chunks to `warehouse` bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4f344-3c5f-440e-82e7-eb55f096a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import s3fs\n",
    "\n",
    "METADATA_PREFIX = \"metadata\"\n",
    "\n",
    "# Using s3fs to save and delete objects from MinIO\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "# Split the documents and save the metadata to warehouse bucket\n",
    "def create_object_task(json_data):\n",
    "    for record in json_data[\"Records\"]:\n",
    "        bucket_name = record[\"s3\"][\"bucket\"][\"name\"]\n",
    "        object_key = urllib.parse.unquote(record[\"s3\"][\"object\"][\"key\"])\n",
    "        print(record[\"s3\"][\"bucket\"][\"name\"],\n",
    "              record[\"s3\"][\"object\"][\"key\"])\n",
    "\n",
    "        doc_splits = split_doc_by_chunks(bucket_name, object_key)\n",
    "\n",
    "        for i, chunk in enumerate(doc_splits):\n",
    "            source = f\"warehouse/{METADATA_PREFIX}/{bucket_name}/{object_key}/chunk_{i:05d}.json\"\n",
    "            with s3.open(source, \"w\") as f:\n",
    "                f.write(chunk.json())\n",
    "    return \"Task completed!\"\n",
    "\n",
    "\n",
    "def delete_object_task(json_data):\n",
    "    for record in json_data[\"Records\"]:\n",
    "        bucket_name = record[\"s3\"][\"bucket\"][\"name\"]\n",
    "        object_key = urllib.parse.unquote(record[\"s3\"][\"object\"][\"key\"])\n",
    "        s3.delete(f\"warehouse/{METADATA_PREFIX}/{bucket_name}/{object_key}\", recursive=True)\n",
    "    return \"Task completed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b770e3-f9d8-4bf7-b554-da1a3851d7b9",
   "metadata": {},
   "source": [
    "## Update FastAPI server with the new logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e551ad8-c03c-4f17-8792-58a85315104a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "import requests\n",
    "from fastapi import FastAPI, Request, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/api/v1/document/notification\")\n",
    "async def receive_webhook(request: Request, background_tasks: BackgroundTasks):\n",
    "    json_data = await request.json()\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectCreated:Put\":\n",
    "        print(\"New object created!\")\n",
    "        background_tasks.add_task(create_object_task, json_data)\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectRemoved:Delete\":\n",
    "        print(\"Object deleted!\")\n",
    "        background_tasks.add_task(delete_object_task, json_data)\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "with gr.Blocks(gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## RAG with MinIO\")\n",
    "    ch_interface = gr.ChatInterface(llm_chat, undo_btn=None, clear_btn=\"Clear\")\n",
    "    ch_interface.chatbot.show_label = False\n",
    "\n",
    "demo.queue()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    app = gr.mount_gradio_app(app, demo, path=CHAT_API_PATH)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8808)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6e3a4-ca84-4a7b-a2e1-b1984e66d1d1",
   "metadata": {},
   "source": [
    "## Add new webhook to process document metadata/chunks\n",
    "\n",
    "Now that we have the first webhook working next step is the get all the chunks with metadata Generate the Embeddings and store it in the vector Database\n",
    "\n",
    "![MinIO-RAG-Ingest.png](media/rag-minio.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a96d42-bc95-449e-a3a6-14bf50c9dc8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "import requests\n",
    "from fastapi import FastAPI, Request, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/api/v1/metadata/notification\")\n",
    "async def receive_metadata_webhook(request: Request, background_tasks: BackgroundTasks):\n",
    "    json_data = await request.json()\n",
    "    print(json.dumps(json_data, indent=2))\n",
    "\n",
    "@app.post(\"/api/v1/document/notification\")\n",
    "async def receive_webhook(request: Request, background_tasks: BackgroundTasks):\n",
    "    json_data = await request.json()\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectCreated:Put\":\n",
    "        print(\"New object created!\")\n",
    "        background_tasks.add_task(create_object_task, json_data)\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectRemoved:Delete\":\n",
    "        print(\"Object deleted!\")\n",
    "        background_tasks.add_task(delete_object_task, json_data)\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "with gr.Blocks(gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## RAG with MinIO\")\n",
    "    ch_interface = gr.ChatInterface(llm_chat, undo_btn=None, clear_btn=\"Clear\")\n",
    "    ch_interface.chatbot.show_label = False\n",
    "\n",
    "demo.queue()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    app = gr.mount_gradio_app(app, demo, path=CHAT_API_PATH)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8808)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493954b-6807-4ea6-9737-fb48d7f1678b",
   "metadata": {},
   "source": [
    "### Create MinIO Event Notifications and Link it to `warehouse` Bucket\n",
    "\n",
    "### Creat Webhook Event\n",
    "\n",
    "In Console go to `Events`-> `Add Event Destination` -> `Webhook`\n",
    "\n",
    "Fill the fields with Following values and hit save\n",
    "\n",
    "**Identifier** - metadata-webhook\n",
    "\n",
    "**Endpoint** - http://localhost:8808/api/v1/metadata/notification\n",
    "\n",
    "Click `Restart MinIO` at the top when pormpted to\n",
    "\n",
    "(**Note**: You can also use `mc` for this)\n",
    "\n",
    "### Link the Webhook Event to `custom-corpus` bucket Events\n",
    "\n",
    "In console go to `Buckets (Administrator)` -> `warehouse` -> `Events`\n",
    "\n",
    "Fill the fields with Following values and hit save\n",
    "\n",
    "**ARN** - Select the `metadata-webhook` from dropdown\n",
    "\n",
    "**Prefix** - `metadata/`\n",
    "\n",
    "**Suffix** - `.json`\n",
    "\n",
    "**Select Events** - Check `PUT` and `DELETE`\n",
    "\n",
    "(**Note**: You can also use `mc` for this)\n",
    "\n",
    "We have our first webhook setup\n",
    "\n",
    "#### Now test by adding and removing an object in `custom-corpus` and see if this webhook gets triggeres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff0d93-9a95-4b78-aa80-252c5e16c061",
   "metadata": {},
   "source": [
    "## Create LanceDB Vector Database in MinIO\n",
    "\n",
    "Now that we have the basic webhook working, lets setup the `lanceDB` vector databse in MinIO `warehouse` bucket in which we will save all the embeddings and additional metadata fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364c30b-9d4b-482b-a310-241893755310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lancedb\n",
    "\n",
    "\n",
    "# Set these environment variables for the lanceDB to connect to MinIO\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = MINIO_ACCESS_KEY\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = MINIO_SECRET_KEY\n",
    "os.environ[\"AWS_ENDPOINT\"] = MINIO_ENDPOINT\n",
    "os.environ[\"ALLOW_HTTP\"] = \"True\"\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"s3://warehouse/v-db/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1e00d-061c-4ed8-b71e-625706299991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list existing tables\n",
    "db.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f7476-e737-4613-a6e5-d3e79a9ea22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table with pydantic schema\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import pyarrow as pa\n",
    "\n",
    "DOCS_TABLE = \"docs\"\n",
    "EMBEDDINGS_DIM = 768\n",
    "\n",
    "table = None\n",
    "\n",
    "\n",
    "class DocsModel(LanceModel):\n",
    "    parent_source: str # Actual object/document source\n",
    "    source: str # Chunk/Metadata source\n",
    "    text: str # Chunked text\n",
    "    vector: Vector(EMBEDDINGS_DIM, pa.float16()) # Vector to be stored\n",
    "\n",
    "\n",
    "def get_or_create_table():\n",
    "    global table\n",
    "    if table is None and DOCS_TABLE not in list(db.table_names()):\n",
    "        return db.create_table(DOCS_TABLE, schema=DocsModel)\n",
    "    if table is None:\n",
    "        table = db.open_table(DOCS_TABLE)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a23ad-b93e-4f3c-bf43-0ce39c5dbed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if that worked\n",
    "get_or_create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2548d9-378c-4a5c-9e86-47d16cdfee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list existing tables\n",
    "db.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b130fb2-784f-4de5-822c-56e2f074d708",
   "metadata": {},
   "source": [
    "## Add Storing/removing data from lanceDB to `metadata-webhook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1bae0-d318-44cd-8e7f-36537c9f3221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "EMBEDDING_DOCUMENT_PREFIX = \"search_document\"\n",
    "\n",
    "# Add queue that keeps the processed meteadata in memory\n",
    "add_data_queue = multiprocessing.Queue()\n",
    "delete_data_queue = multiprocessing.Queue()\n",
    "\n",
    "def create_metadata_task(json_data):\n",
    "    for record in json_data[\"Records\"]:\n",
    "        bucket_name = record[\"s3\"][\"bucket\"][\"name\"]\n",
    "        object_key = urllib.parse.unquote(record[\"s3\"][\"object\"][\"key\"])\n",
    "        print(bucket_name,\n",
    "              object_key)\n",
    "        with s3.open(f\"{bucket_name}/{object_key}\", \"r\") as f:\n",
    "            data = f.read()\n",
    "            chunk_json = json.loads(data)\n",
    "            embeddings = get_embedding(f\"{EMBEDDING_DOCUMENT_PREFIX}: {chunk_json['page_content']}\")\n",
    "            add_data_queue.put({\n",
    "                \"text\": chunk_json[\"page_content\"],\n",
    "                \"parent_source\": chunk_json.get(\"metadata\", \"\").get(\"source\", \"\"),\n",
    "                \"source\": f\"{bucket_name}/{object_key}\",\n",
    "                \"vector\": embeddings\n",
    "            })\n",
    "    return \"Metadata Create Task Completed!\"\n",
    "\n",
    "\n",
    "def delete_metadata_task(json_data):\n",
    "    for record in json_data[\"Records\"]:\n",
    "        bucket_name = record[\"s3\"][\"bucket\"][\"name\"]\n",
    "        object_key = urllib.parse.unquote(record[\"s3\"][\"object\"][\"key\"])\n",
    "        delete_data_queue.put(f\"{bucket_name}/{object_key}\")\n",
    "    return \"Metadata Delete Task completed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49154d2-dcb0-4319-933a-f331ac8297dc",
   "metadata": {},
   "source": [
    "## Add a scheduler that Processes Data from Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1812e4-28a1-4d5a-a861-a283a25f9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import pandas as pd\n",
    "\n",
    "def add_vector_job():\n",
    "    data = []\n",
    "    table = get_or_create_table()\n",
    "\n",
    "    while not add_data_queue.empty():\n",
    "        item = add_data_queue.get()\n",
    "        data.append(item)\n",
    "\n",
    "    if len(data) > 0:\n",
    "        df = pd.DataFrame(data)\n",
    "        table.add(df)\n",
    "        table.compact_files()\n",
    "        print(len(table.to_pandas()))\n",
    "\n",
    "\n",
    "def delete_vector_job():\n",
    "    table = get_or_create_table()\n",
    "    source_data = []\n",
    "    while not delete_data_queue.empty():\n",
    "        item = delete_data_queue.get()\n",
    "        source_data.append(item)\n",
    "    if len(source_data) > 0:\n",
    "        filter_data = \", \".join([f'\"{d}\"' for d in source_data])\n",
    "        table.delete(f'source IN ({filter_data})')\n",
    "        table.compact_files()\n",
    "        table.cleanup_old_versions()\n",
    "        print(len(table.to_pandas()))\n",
    "\n",
    "\n",
    "scheduler = BackgroundScheduler()\n",
    "\n",
    "scheduler.add_job(add_vector_job, 'interval', seconds=10)\n",
    "scheduler.add_job(delete_vector_job, 'interval', seconds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8c664-d319-4454-b9e2-8821ae34da64",
   "metadata": {},
   "source": [
    "## Update FastAPI with the Vector Embedding Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e26ca0-7b21-404f-9acd-d7540fa7a987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "import requests\n",
    "from fastapi import FastAPI, Request, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    get_or_create_table()\n",
    "    if not scheduler.running:\n",
    "        scheduler.start()\n",
    "\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    scheduler.shutdown()\n",
    "\n",
    "@app.post(\"/api/v1/metadata/notification\")\n",
    "async def receive_metadata_webhook(request: Request, background_tasks: BackgroundTasks):\n",
    "    json_data = await request.json()\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectCreated:Put\":\n",
    "        print(\"New Metadata created!\")\n",
    "        background_tasks.add_task(create_metadata_task, json_data)\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectRemoved:Delete\":\n",
    "        print(\"Metadata deleted!\")\n",
    "        background_tasks.add_task(delete_metadata_task, json_data)\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "@app.post(\"/api/v1/document/notification\")\n",
    "async def receive_webhook(request: Request, background_tasks: BackgroundTasks):\n",
    "    json_data = await request.json()\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectCreated:Put\":\n",
    "        print(\"New object created!\")\n",
    "        background_tasks.add_task(create_object_task, json_data)\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectRemoved:Delete\":\n",
    "        print(\"Object deleted!\")\n",
    "        background_tasks.add_task(delete_object_task, json_data)\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "with gr.Blocks(gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## RAG with MinIO\")\n",
    "    ch_interface = gr.ChatInterface(llm_chat, undo_btn=None, clear_btn=\"Clear\")\n",
    "    ch_interface.chatbot.show_label = False\n",
    "    ch_interface.chatbot.height = 600\n",
    "\n",
    "demo.queue()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    app = gr.mount_gradio_app(app, demo, path=CHAT_API_PATH)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8808)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de82d6f-e2ba-442f-a239-0b1f2486eb16",
   "metadata": {},
   "source": [
    "## Recap\n",
    "![MinIO-RAG-Ingest.png](media/rag-minio.jpg)\n",
    "\n",
    "Now that we have the Ingestion pipeline wroking let's integrate the final RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b15feb-db5d-4081-82ea-05d5eacf8680",
   "metadata": {},
   "source": [
    "## Add Vector Search Capability\n",
    "\n",
    "Now that we have the document ingested into the lanceDB let's add the search capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ed418-64d0-401d-b0d1-97c286d601d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_QUERY_PREFIX = \"search_query\"\n",
    "\n",
    "def search(query, limit=5):\n",
    "    query_embedding = get_embedding(f\"{EMBEDDING_QUERY_PREFIX}: {query}\")\n",
    "    res = get_or_create_table().search(query_embedding).metric(\"cosine\").limit(limit)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2f714-fed5-4569-a289-3bb61879ac77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets test to see if it works\n",
    "\n",
    "res = search(\"What is MinIO Enterprise Object Store Lite?\")\n",
    "res.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b9119-3618-4160-b0b9-3c53713554da",
   "metadata": {},
   "source": [
    "## Prompt LLM to use the Relevant Documets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b748cb-a5de-430f-8e6e-2834fc9a0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = \"\"\"\n",
    "DOCUMENT:\n",
    "{documents}\n",
    "\n",
    "QUESTION:\n",
    "{user_question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Answer in detail the user's QUESTION using the DOCUMENT text above.\n",
    "Keep your answer ground in the facts of the DOCUMENT. Do not use sentence like \"The document states\" citing the document.\n",
    "If the DOCUMENT doesn't contain the facts to answer the QUESTION only Respond with \"Sorry! I Don't know\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7f768-492a-4738-aa51-75e6714c331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_df = []\n",
    "\n",
    "def llm_chat(user_question, history):\n",
    "    history = history or []\n",
    "    global context_df\n",
    "    # Search for relevant document chunks\n",
    "    res = search(user_question)\n",
    "    documents = \" \".join([d[\"text\"].strip() for d in res.to_list()]) \n",
    "    # Pass the chunks to LLM for grounded response\n",
    "    llm_resp = requests.post(LLM_ENDPOINT,\n",
    "                             json={\"model\": LLM_MODEL,\n",
    "                                   \"messages\": [\n",
    "                                       {\"role\": \"user\",\n",
    "                                        \"content\": RAG_PROMPT.format(user_question=user_question, documents=documents)\n",
    "                                        }\n",
    "                                   ],\n",
    "                                   \"options\": {\n",
    "                                       \"temperature\": 0,\n",
    "                                       \"top_p\": 0.90,\n",
    "                                   }},\n",
    "                             stream=True)\n",
    "    bot_response = \"**AI:** \"\n",
    "    for resp in llm_resp.iter_lines():\n",
    "        json_data = json.loads(resp)\n",
    "        bot_response += json_data[\"message\"][\"content\"]\n",
    "        yield bot_response\n",
    "    context_df = res.to_pandas()\n",
    "    context_df = context_df.drop(columns=['source', 'vector'])\n",
    "\n",
    "\n",
    "def clear_events():\n",
    "    global context_df\n",
    "    context_df = []\n",
    "    return context_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140cc902-5c08-4047-a9a7-365fb66f8f1d",
   "metadata": {},
   "source": [
    "## Update FastAPI Chat Endpoint to use RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef8499-0fe6-4396-b393-a20a396cd854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "import requests\n",
    "from fastapi import FastAPI, Request, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    get_or_create_table()\n",
    "    if not scheduler.running:\n",
    "        scheduler.start()\n",
    "\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    scheduler.shutdown()\n",
    "\n",
    "@app.post(\"/api/v1/metadata/notification\")\n",
    "async def receive_metadata_webhook(request: Request, background_tasks: BackgroundTasks):\n",
    "    json_data = await request.json()\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectCreated:Put\":\n",
    "        print(\"New Metadata created!\")\n",
    "        background_tasks.add_task(create_metadata_task, json_data)\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectRemoved:Delete\":\n",
    "        print(\"Metadata deleted!\")\n",
    "        background_tasks.add_task(delete_metadata_task, json_data)\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "@app.post(\"/api/v1/document/notification\")\n",
    "async def receive_webhook(request: Request, background_tasks: BackgroundTasks):\n",
    "    json_data = await request.json()\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectCreated:Put\":\n",
    "        print(\"New object created!\")\n",
    "        background_tasks.add_task(create_object_task, json_data)\n",
    "    if json_data[\"EventName\"] == \"s3:ObjectRemoved:Delete\":\n",
    "        print(\"Object deleted!\")\n",
    "        background_tasks.add_task(delete_object_task, json_data)\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "with gr.Blocks(gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## RAG with MinIO\")\n",
    "    ch_interface = gr.ChatInterface(llm_chat, undo_btn=None, clear_btn=\"Clear\")\n",
    "    ch_interface.chatbot.show_label = False\n",
    "    ch_interface.chatbot.height = 600\n",
    "    gr.Markdown(\"### Context Supplied\")\n",
    "    context_dataframe = gr.DataFrame(headers=[\"parent_source\", \"text\", \"_distance\"], wrap=True)\n",
    "    ch_interface.clear_btn.click(clear_events, [], context_dataframe)\n",
    "\n",
    "    @gr.on(ch_interface.output_components, inputs=[ch_interface.chatbot], outputs=[context_dataframe])\n",
    "    def update_chat_context_df(text):\n",
    "        global context_df\n",
    "        if context_df is not None:\n",
    "            return context_df\n",
    "        return \"\"\n",
    "\n",
    "demo.queue()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    app = gr.mount_gradio_app(app, demo, path=CHAT_API_PATH)\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8808)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba846b-cfe9-46b9-807b-d5f85ae7f008",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
